{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from jinja2->torch==2.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from sympy->torch==2.2) (1.3.0)\n",
      "Requirement already satisfied: torchtext==0.17 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: tqdm in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchtext==0.17) (4.66.2)\n",
      "Requirement already satisfied: requests in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchtext==0.17) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchtext==0.17) (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchtext==0.17) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchtext==0.17) (0.7.1)\n",
      "Requirement already satisfied: filelock in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torch==2.2.0->torchtext==0.17) (2024.3.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from torchdata==0.7.1->torchtext==0.17) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from requests->torchtext==0.17) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from requests->torchtext==0.17) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from requests->torchtext==0.17) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from jinja2->torch==2.2.0->torchtext==0.17) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages (from sympy->torch==2.2.0->torchtext==0.17) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2\n",
    "!pip install torchtext==0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op\n",
    "\n",
    "def gen_sqr_nxt_mask(size):\n",
    "    msk = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_iter = PennTreebank(split='train')\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "tr_iter, val_iter, te_iter = PennTreebank()\n",
    "training_text = process_data(tr_iter)\n",
    "validation_text = process_data(val_iter)\n",
    "testing_text = process_data(te_iter)\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koyiljonvaliev/Anaconda/anaconda3/envs/TorchEnv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocabulary) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    num_batches = len(training_data) // max_seq_len\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        sequence_length = train_data_batch.size(0)\n",
    "        if sequence_length != max_seq_len:  # only on last batch\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_model(train_data_batch, mask_source)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        optim_module.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            sequence_length = eval_data.size(0)\n",
    "            if sequence_length != max_seq_len:\n",
    "                mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "            op = eval_model_obj(eval_data, mask_source)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += sequence_length * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/451 batches, training loss 7.83, training perplexity 2524.45\n",
      "epoch 1, 200/451 batches, training loss 6.41, training perplexity 606.97\n",
      "epoch 1, 300/451 batches, training loss 6.00, training perplexity 403.64\n",
      "epoch 1, 400/451 batches, training loss 5.81, training perplexity 334.19\n",
      "\n",
      "epoch 1, validation loss 5.68, validation perplexity 292.51\n",
      "\n",
      "epoch 2, 100/451 batches, training loss 5.66, training perplexity 285.86\n",
      "epoch 2, 200/451 batches, training loss 5.56, training perplexity 260.93\n",
      "epoch 2, 300/451 batches, training loss 5.43, training perplexity 228.38\n",
      "epoch 2, 400/451 batches, training loss 5.35, training perplexity 210.83\n",
      "\n",
      "epoch 2, validation loss 5.37, validation perplexity 215.56\n",
      "\n",
      "epoch 3, 100/451 batches, training loss 5.35, training perplexity 211.18\n",
      "epoch 3, 200/451 batches, training loss 5.29, training perplexity 199.19\n",
      "epoch 3, 300/451 batches, training loss 5.18, training perplexity 177.92\n",
      "epoch 3, 400/451 batches, training loss 5.12, training perplexity 167.48\n",
      "\n",
      "epoch 3, validation loss 5.21, validation perplexity 182.35\n",
      "\n",
      "epoch 4, 100/451 batches, training loss 5.16, training perplexity 175.00\n",
      "epoch 4, 200/451 batches, training loss 5.12, training perplexity 167.96\n",
      "epoch 4, 300/451 batches, training loss 5.02, training perplexity 150.95\n",
      "epoch 4, 400/451 batches, training loss 4.96, training perplexity 142.53\n",
      "\n",
      "epoch 4, validation loss 5.14, validation perplexity 171.15\n",
      "\n",
      "epoch 5, 100/451 batches, training loss 5.03, training perplexity 153.02\n",
      "epoch 5, 200/451 batches, training loss 5.00, training perplexity 148.07\n",
      "epoch 5, 300/451 batches, training loss 4.90, training perplexity 133.72\n",
      "epoch 5, 400/451 batches, training loss 4.84, training perplexity 126.19\n",
      "\n",
      "epoch 5, validation loss 5.09, validation perplexity 162.33\n",
      "\n",
      "epoch 6, 100/451 batches, training loss 4.93, training perplexity 137.70\n",
      "epoch 6, 200/451 batches, training loss 4.89, training perplexity 133.38\n",
      "epoch 6, 300/451 batches, training loss 4.80, training perplexity 121.35\n",
      "epoch 6, 400/451 batches, training loss 4.74, training perplexity 113.95\n",
      "\n",
      "epoch 6, validation loss 5.03, validation perplexity 152.49\n",
      "\n",
      "epoch 7, 100/451 batches, training loss 4.83, training perplexity 125.75\n",
      "epoch 7, 200/451 batches, training loss 4.81, training perplexity 122.31\n",
      "epoch 7, 300/451 batches, training loss 4.71, training perplexity 111.40\n",
      "epoch 7, 400/451 batches, training loss 4.65, training perplexity 104.50\n",
      "\n",
      "epoch 7, validation loss 4.99, validation perplexity 147.31\n",
      "\n",
      "epoch 8, 100/451 batches, training loss 4.76, training perplexity 117.03\n",
      "epoch 8, 200/451 batches, training loss 4.73, training perplexity 113.76\n",
      "epoch 8, 300/451 batches, training loss 4.64, training perplexity 103.85\n",
      "epoch 8, 400/451 batches, training loss 4.58, training perplexity 97.05\n",
      "\n",
      "epoch 8, validation loss 4.98, validation perplexity 145.18\n",
      "\n",
      "epoch 9, 100/451 batches, training loss 4.70, training perplexity 109.93\n",
      "epoch 9, 200/451 batches, training loss 4.67, training perplexity 106.63\n",
      "epoch 9, 300/451 batches, training loss 4.58, training perplexity 97.54\n",
      "epoch 9, 400/451 batches, training loss 4.51, training perplexity 91.28\n",
      "\n",
      "epoch 9, validation loss 4.95, validation perplexity 141.83\n",
      "\n",
      "epoch 10, 100/451 batches, training loss 4.64, training perplexity 103.36\n",
      "epoch 10, 200/451 batches, training loss 4.61, training perplexity 100.94\n",
      "epoch 10, 300/451 batches, training loss 4.52, training perplexity 92.12\n",
      "epoch 10, 400/451 batches, training loss 4.46, training perplexity 86.26\n",
      "\n",
      "epoch 10, validation loss 4.94, validation perplexity 140.02\n",
      "\n",
      "epoch 11, 100/451 batches, training loss 4.59, training perplexity 98.80\n",
      "epoch 11, 200/451 batches, training loss 4.56, training perplexity 95.89\n",
      "epoch 11, 300/451 batches, training loss 4.48, training perplexity 87.84\n",
      "epoch 11, 400/451 batches, training loss 4.41, training perplexity 82.21\n",
      "\n",
      "epoch 11, validation loss 4.93, validation perplexity 138.44\n",
      "\n",
      "epoch 12, 100/451 batches, training loss 4.54, training perplexity 94.16\n",
      "epoch 12, 200/451 batches, training loss 4.52, training perplexity 91.96\n",
      "epoch 12, 300/451 batches, training loss 4.43, training perplexity 84.04\n",
      "epoch 12, 400/451 batches, training loss 4.36, training perplexity 78.52\n",
      "\n",
      "epoch 12, validation loss 4.92, validation perplexity 136.85\n",
      "\n",
      "epoch 13, 100/451 batches, training loss 4.51, training perplexity 90.89\n",
      "epoch 13, 200/451 batches, training loss 4.48, training perplexity 88.54\n",
      "epoch 13, 300/451 batches, training loss 4.39, training perplexity 80.98\n",
      "epoch 13, 400/451 batches, training loss 4.33, training perplexity 75.71\n",
      "\n",
      "epoch 13, validation loss 4.91, validation perplexity 135.57\n",
      "\n",
      "epoch 14, 100/451 batches, training loss 4.48, training perplexity 88.03\n",
      "epoch 14, 200/451 batches, training loss 4.45, training perplexity 85.90\n",
      "epoch 14, 300/451 batches, training loss 4.36, training perplexity 78.36\n",
      "epoch 14, 400/451 batches, training loss 4.30, training perplexity 73.44\n",
      "\n",
      "epoch 14, validation loss 4.90, validation perplexity 134.22\n",
      "\n",
      "epoch 15, 100/451 batches, training loss 4.45, training perplexity 85.45\n",
      "epoch 15, 200/451 batches, training loss 4.42, training perplexity 83.20\n",
      "epoch 15, 300/451 batches, training loss 4.34, training perplexity 76.60\n",
      "epoch 15, 400/451 batches, training loss 4.27, training perplexity 71.25\n",
      "\n",
      "epoch 15, validation loss 4.90, validation perplexity 134.06\n",
      "\n",
      "epoch 16, 100/451 batches, training loss 4.42, training perplexity 83.14\n",
      "epoch 16, 200/451 batches, training loss 4.39, training perplexity 80.98\n",
      "epoch 16, 300/451 batches, training loss 4.31, training perplexity 74.50\n",
      "epoch 16, 400/451 batches, training loss 4.24, training perplexity 69.38\n",
      "\n",
      "epoch 16, validation loss 4.89, validation perplexity 133.11\n",
      "\n",
      "epoch 17, 100/451 batches, training loss 4.40, training perplexity 81.47\n",
      "epoch 17, 200/451 batches, training loss 4.38, training perplexity 79.62\n",
      "epoch 17, 300/451 batches, training loss 4.29, training perplexity 72.73\n",
      "epoch 17, 400/451 batches, training loss 4.22, training perplexity 68.05\n",
      "\n",
      "epoch 17, validation loss 4.89, validation perplexity 132.87\n",
      "\n",
      "epoch 18, 100/451 batches, training loss 4.38, training perplexity 79.87\n",
      "epoch 18, 200/451 batches, training loss 4.36, training perplexity 78.13\n",
      "epoch 18, 300/451 batches, training loss 4.27, training perplexity 71.62\n",
      "epoch 18, 400/451 batches, training loss 4.20, training perplexity 66.83\n",
      "\n",
      "epoch 18, validation loss 4.89, validation perplexity 132.60\n",
      "\n",
      "epoch 19, 100/451 batches, training loss 4.36, training perplexity 78.44\n",
      "epoch 19, 200/451 batches, training loss 4.34, training perplexity 76.45\n",
      "epoch 19, 300/451 batches, training loss 4.25, training perplexity 70.23\n",
      "epoch 19, 400/451 batches, training loss 4.18, training perplexity 65.67\n",
      "\n",
      "epoch 19, validation loss 4.88, validation perplexity 132.27\n",
      "\n",
      "epoch 20, 100/451 batches, training loss 4.35, training perplexity 77.39\n",
      "epoch 20, 200/451 batches, training loss 4.33, training perplexity 75.57\n",
      "epoch 20, 300/451 batches, training loss 4.24, training perplexity 69.41\n",
      "epoch 20, 400/451 batches, training loss 4.17, training perplexity 64.64\n",
      "\n",
      "epoch 20, validation loss 4.88, validation perplexity 131.96\n",
      "\n",
      "epoch 21, 100/451 batches, training loss 4.33, training perplexity 76.27\n",
      "epoch 21, 200/451 batches, training loss 4.31, training perplexity 74.50\n",
      "epoch 21, 300/451 batches, training loss 4.23, training perplexity 68.49\n",
      "epoch 21, 400/451 batches, training loss 4.16, training perplexity 63.84\n",
      "\n",
      "epoch 21, validation loss 4.88, validation perplexity 131.91\n",
      "\n",
      "epoch 22, 100/451 batches, training loss 4.32, training perplexity 75.42\n",
      "epoch 22, 200/451 batches, training loss 4.30, training perplexity 73.86\n",
      "epoch 22, 300/451 batches, training loss 4.22, training perplexity 67.74\n",
      "epoch 22, 400/451 batches, training loss 4.14, training perplexity 63.03\n",
      "\n",
      "epoch 22, validation loss 4.88, validation perplexity 131.52\n",
      "\n",
      "epoch 23, 100/451 batches, training loss 4.31, training perplexity 74.70\n",
      "epoch 23, 200/451 batches, training loss 4.29, training perplexity 72.89\n",
      "epoch 23, 300/451 batches, training loss 4.21, training perplexity 67.04\n",
      "epoch 23, 400/451 batches, training loss 4.13, training perplexity 62.40\n",
      "\n",
      "epoch 23, validation loss 4.88, validation perplexity 131.85\n",
      "\n",
      "epoch 24, 100/451 batches, training loss 4.31, training perplexity 74.08\n",
      "epoch 24, 200/451 batches, training loss 4.28, training perplexity 72.22\n",
      "epoch 24, 300/451 batches, training loss 4.20, training perplexity 66.50\n",
      "epoch 24, 400/451 batches, training loss 4.12, training perplexity 61.71\n",
      "\n",
      "epoch 24, validation loss 4.88, validation perplexity 131.32\n",
      "\n",
      "epoch 25, 100/451 batches, training loss 4.30, training perplexity 73.64\n",
      "epoch 25, 200/451 batches, training loss 4.27, training perplexity 71.82\n",
      "epoch 25, 300/451 batches, training loss 4.19, training perplexity 66.08\n",
      "epoch 25, 400/451 batches, training loss 4.12, training perplexity 61.40\n",
      "\n",
      "epoch 25, validation loss 4.88, validation perplexity 131.54\n",
      "\n",
      "epoch 26, 100/451 batches, training loss 4.29, training perplexity 72.90\n",
      "epoch 26, 200/451 batches, training loss 4.27, training perplexity 71.25\n",
      "epoch 26, 300/451 batches, training loss 4.18, training perplexity 65.45\n",
      "epoch 26, 400/451 batches, training loss 4.11, training perplexity 61.07\n",
      "\n",
      "epoch 26, validation loss 4.88, validation perplexity 131.08\n",
      "\n",
      "epoch 27, 100/451 batches, training loss 4.28, training perplexity 72.48\n",
      "epoch 27, 200/451 batches, training loss 4.26, training perplexity 70.74\n",
      "epoch 27, 300/451 batches, training loss 4.18, training perplexity 65.24\n",
      "epoch 27, 400/451 batches, training loss 4.10, training perplexity 60.55\n",
      "\n",
      "epoch 27, validation loss 4.87, validation perplexity 130.87\n",
      "\n",
      "epoch 28, 100/451 batches, training loss 4.28, training perplexity 72.11\n",
      "epoch 28, 200/451 batches, training loss 4.25, training perplexity 70.31\n",
      "epoch 28, 300/451 batches, training loss 4.17, training perplexity 64.60\n",
      "epoch 28, 400/451 batches, training loss 4.10, training perplexity 60.36\n",
      "\n",
      "epoch 28, validation loss 4.87, validation perplexity 130.79\n",
      "\n",
      "epoch 29, 100/451 batches, training loss 4.27, training perplexity 71.79\n",
      "epoch 29, 200/451 batches, training loss 4.25, training perplexity 70.09\n",
      "epoch 29, 300/451 batches, training loss 4.16, training perplexity 64.26\n",
      "epoch 29, 400/451 batches, training loss 4.09, training perplexity 60.00\n",
      "\n",
      "epoch 29, validation loss 4.87, validation perplexity 130.92\n",
      "\n",
      "epoch 30, 100/451 batches, training loss 4.27, training perplexity 71.38\n",
      "epoch 30, 200/451 batches, training loss 4.25, training perplexity 69.82\n",
      "epoch 30, 300/451 batches, training loss 4.16, training perplexity 64.15\n",
      "epoch 30, 400/451 batches, training loss 4.09, training perplexity 59.64\n",
      "\n",
      "epoch 30, validation loss 4.87, validation perplexity 130.84\n",
      "\n",
      "epoch 31, 100/451 batches, training loss 4.27, training perplexity 71.24\n",
      "epoch 31, 200/451 batches, training loss 4.24, training perplexity 69.57\n",
      "epoch 31, 300/451 batches, training loss 4.16, training perplexity 63.89\n",
      "epoch 31, 400/451 batches, training loss 4.09, training perplexity 59.51\n",
      "\n",
      "epoch 31, validation loss 4.87, validation perplexity 130.63\n",
      "\n",
      "epoch 32, 100/451 batches, training loss 4.26, training perplexity 70.89\n",
      "epoch 32, 200/451 batches, training loss 4.24, training perplexity 69.51\n",
      "epoch 32, 300/451 batches, training loss 4.16, training perplexity 63.80\n",
      "epoch 32, 400/451 batches, training loss 4.08, training perplexity 59.30\n",
      "\n",
      "epoch 32, validation loss 4.87, validation perplexity 130.69\n",
      "\n",
      "epoch 33, 100/451 batches, training loss 4.26, training perplexity 70.69\n",
      "epoch 33, 200/451 batches, training loss 4.23, training perplexity 69.00\n",
      "epoch 33, 300/451 batches, training loss 4.15, training perplexity 63.66\n",
      "epoch 33, 400/451 batches, training loss 4.08, training perplexity 59.23\n",
      "\n",
      "epoch 33, validation loss 4.87, validation perplexity 130.39\n",
      "\n",
      "epoch 34, 100/451 batches, training loss 4.26, training perplexity 70.50\n",
      "epoch 34, 200/451 batches, training loss 4.23, training perplexity 68.93\n",
      "epoch 34, 300/451 batches, training loss 4.15, training perplexity 63.45\n",
      "epoch 34, 400/451 batches, training loss 4.08, training perplexity 59.11\n",
      "\n",
      "epoch 34, validation loss 4.87, validation perplexity 130.44\n",
      "\n",
      "epoch 35, 100/451 batches, training loss 4.26, training perplexity 70.47\n",
      "epoch 35, 200/451 batches, training loss 4.24, training perplexity 69.14\n",
      "epoch 35, 300/451 batches, training loss 4.15, training perplexity 63.36\n",
      "epoch 35, 400/451 batches, training loss 4.08, training perplexity 59.04\n",
      "\n",
      "epoch 35, validation loss 4.87, validation perplexity 130.49\n",
      "\n",
      "epoch 36, 100/451 batches, training loss 4.25, training perplexity 70.26\n",
      "epoch 36, 200/451 batches, training loss 4.23, training perplexity 68.71\n",
      "epoch 36, 300/451 batches, training loss 4.15, training perplexity 63.30\n",
      "epoch 36, 400/451 batches, training loss 4.08, training perplexity 58.96\n",
      "\n",
      "epoch 36, validation loss 4.87, validation perplexity 130.31\n",
      "\n",
      "epoch 37, 100/451 batches, training loss 4.25, training perplexity 70.16\n",
      "epoch 37, 200/451 batches, training loss 4.23, training perplexity 68.60\n",
      "epoch 37, 300/451 batches, training loss 4.14, training perplexity 63.00\n",
      "epoch 37, 400/451 batches, training loss 4.07, training perplexity 58.64\n",
      "\n",
      "epoch 37, validation loss 4.87, validation perplexity 130.37\n",
      "\n",
      "epoch 38, 100/451 batches, training loss 4.25, training perplexity 70.18\n",
      "epoch 38, 200/451 batches, training loss 4.23, training perplexity 68.56\n",
      "epoch 38, 300/451 batches, training loss 4.14, training perplexity 62.96\n",
      "epoch 38, 400/451 batches, training loss 4.07, training perplexity 58.77\n",
      "\n",
      "epoch 38, validation loss 4.87, validation perplexity 130.30\n",
      "\n",
      "epoch 39, 100/451 batches, training loss 4.25, training perplexity 69.98\n",
      "epoch 39, 200/451 batches, training loss 4.22, training perplexity 68.17\n",
      "epoch 39, 300/451 batches, training loss 4.14, training perplexity 62.84\n",
      "epoch 39, 400/451 batches, training loss 4.07, training perplexity 58.61\n",
      "\n",
      "epoch 39, validation loss 4.87, validation perplexity 130.32\n",
      "\n",
      "epoch 40, 100/451 batches, training loss 4.25, training perplexity 69.80\n",
      "epoch 40, 200/451 batches, training loss 4.23, training perplexity 68.44\n",
      "epoch 40, 300/451 batches, training loss 4.14, training perplexity 62.82\n",
      "epoch 40, 400/451 batches, training loss 4.07, training perplexity 58.34\n",
      "\n",
      "epoch 40, validation loss 4.87, validation perplexity 130.26\n",
      "\n",
      "epoch 41, 100/451 batches, training loss 4.25, training perplexity 69.84\n",
      "epoch 41, 200/451 batches, training loss 4.22, training perplexity 67.95\n",
      "epoch 41, 300/451 batches, training loss 4.14, training perplexity 62.71\n",
      "epoch 41, 400/451 batches, training loss 4.07, training perplexity 58.50\n",
      "\n",
      "epoch 41, validation loss 4.87, validation perplexity 130.24\n",
      "\n",
      "epoch 42, 100/451 batches, training loss 4.25, training perplexity 69.80\n",
      "epoch 42, 200/451 batches, training loss 4.23, training perplexity 68.41\n",
      "epoch 42, 300/451 batches, training loss 4.13, training perplexity 62.47\n",
      "epoch 42, 400/451 batches, training loss 4.06, training perplexity 58.23\n",
      "\n",
      "epoch 42, validation loss 4.87, validation perplexity 130.24\n",
      "\n",
      "epoch 43, 100/451 batches, training loss 4.24, training perplexity 69.37\n",
      "epoch 43, 200/451 batches, training loss 4.22, training perplexity 68.07\n",
      "epoch 43, 300/451 batches, training loss 4.14, training perplexity 62.61\n",
      "epoch 43, 400/451 batches, training loss 4.07, training perplexity 58.41\n",
      "\n",
      "epoch 43, validation loss 4.87, validation perplexity 130.28\n",
      "\n",
      "epoch 44, 100/451 batches, training loss 4.24, training perplexity 69.53\n",
      "epoch 44, 200/451 batches, training loss 4.22, training perplexity 67.95\n",
      "epoch 44, 300/451 batches, training loss 4.13, training perplexity 62.43\n",
      "epoch 44, 400/451 batches, training loss 4.07, training perplexity 58.27\n",
      "\n",
      "epoch 44, validation loss 4.87, validation perplexity 130.26\n",
      "\n",
      "epoch 45, 100/451 batches, training loss 4.24, training perplexity 69.53\n",
      "epoch 45, 200/451 batches, training loss 4.22, training perplexity 68.13\n",
      "epoch 45, 300/451 batches, training loss 4.14, training perplexity 62.53\n",
      "epoch 45, 400/451 batches, training loss 4.07, training perplexity 58.30\n",
      "\n",
      "epoch 45, validation loss 4.87, validation perplexity 130.31\n",
      "\n",
      "epoch 46, 100/451 batches, training loss 4.24, training perplexity 69.40\n",
      "epoch 46, 200/451 batches, training loss 4.22, training perplexity 67.94\n",
      "epoch 46, 300/451 batches, training loss 4.13, training perplexity 62.43\n",
      "epoch 46, 400/451 batches, training loss 4.06, training perplexity 58.10\n",
      "\n",
      "epoch 46, validation loss 4.87, validation perplexity 130.23\n",
      "\n",
      "epoch 47, 100/451 batches, training loss 4.24, training perplexity 69.49\n",
      "epoch 47, 200/451 batches, training loss 4.22, training perplexity 67.83\n",
      "epoch 47, 300/451 batches, training loss 4.13, training perplexity 62.38\n",
      "epoch 47, 400/451 batches, training loss 4.06, training perplexity 58.23\n",
      "\n",
      "epoch 47, validation loss 4.87, validation perplexity 130.19\n",
      "\n",
      "epoch 48, 100/451 batches, training loss 4.24, training perplexity 69.44\n",
      "epoch 48, 200/451 batches, training loss 4.22, training perplexity 67.87\n",
      "epoch 48, 300/451 batches, training loss 4.13, training perplexity 62.15\n",
      "epoch 48, 400/451 batches, training loss 4.06, training perplexity 58.07\n",
      "\n",
      "epoch 48, validation loss 4.87, validation perplexity 130.24\n",
      "\n",
      "epoch 49, 100/451 batches, training loss 4.24, training perplexity 69.25\n",
      "epoch 49, 200/451 batches, training loss 4.22, training perplexity 67.90\n",
      "epoch 49, 300/451 batches, training loss 4.13, training perplexity 62.26\n",
      "epoch 49, 400/451 batches, training loss 4.06, training perplexity 58.26\n",
      "\n",
      "epoch 49, validation loss 4.87, validation perplexity 130.19\n",
      "\n",
      "epoch 50, 100/451 batches, training loss 4.24, training perplexity 69.37\n",
      "epoch 50, 200/451 batches, training loss 4.22, training perplexity 67.81\n",
      "epoch 50, 300/451 batches, training loss 4.13, training perplexity 62.30\n",
      "epoch 50, 400/451 batches, training loss 4.06, training perplexity 58.15\n",
      "\n",
      "epoch 50, validation loss 4.87, validation perplexity 130.16\n",
      "\n",
      "epoch 51, 100/451 batches, training loss 4.24, training perplexity 69.16\n",
      "epoch 51, 200/451 batches, training loss 4.22, training perplexity 68.04\n",
      "epoch 51, 300/451 batches, training loss 4.13, training perplexity 62.33\n",
      "epoch 51, 400/451 batches, training loss 4.06, training perplexity 58.06\n",
      "\n",
      "epoch 51, validation loss 4.87, validation perplexity 130.23\n",
      "\n",
      "epoch 52, 100/451 batches, training loss 4.24, training perplexity 69.29\n",
      "epoch 52, 200/451 batches, training loss 4.22, training perplexity 67.89\n",
      "epoch 52, 300/451 batches, training loss 4.13, training perplexity 62.09\n",
      "epoch 52, 400/451 batches, training loss 4.06, training perplexity 58.08\n",
      "\n",
      "epoch 52, validation loss 4.87, validation perplexity 130.20\n",
      "\n",
      "epoch 53, 100/451 batches, training loss 4.24, training perplexity 69.28\n",
      "epoch 53, 200/451 batches, training loss 4.22, training perplexity 67.88\n",
      "epoch 53, 300/451 batches, training loss 4.13, training perplexity 62.17\n",
      "epoch 53, 400/451 batches, training loss 4.06, training perplexity 58.20\n",
      "\n",
      "epoch 53, validation loss 4.87, validation perplexity 130.15\n",
      "\n",
      "epoch 54, 100/451 batches, training loss 4.24, training perplexity 69.36\n",
      "epoch 54, 200/451 batches, training loss 4.21, training perplexity 67.57\n",
      "epoch 54, 300/451 batches, training loss 4.13, training perplexity 62.31\n",
      "epoch 54, 400/451 batches, training loss 4.06, training perplexity 58.00\n",
      "\n",
      "epoch 54, validation loss 4.87, validation perplexity 130.14\n",
      "\n",
      "epoch 55, 100/451 batches, training loss 4.24, training perplexity 69.14\n",
      "epoch 55, 200/451 batches, training loss 4.21, training perplexity 67.67\n",
      "epoch 55, 300/451 batches, training loss 4.13, training perplexity 62.26\n",
      "epoch 55, 400/451 batches, training loss 4.06, training perplexity 58.03\n",
      "\n",
      "epoch 55, validation loss 4.87, validation perplexity 130.15\n",
      "\n",
      "epoch 56, 100/451 batches, training loss 4.24, training perplexity 69.33\n",
      "epoch 56, 200/451 batches, training loss 4.22, training perplexity 67.80\n",
      "epoch 56, 300/451 batches, training loss 4.13, training perplexity 62.04\n",
      "epoch 56, 400/451 batches, training loss 4.06, training perplexity 58.12\n",
      "\n",
      "epoch 56, validation loss 4.87, validation perplexity 130.14\n",
      "\n",
      "epoch 57, 100/451 batches, training loss 4.24, training perplexity 69.35\n",
      "epoch 57, 200/451 batches, training loss 4.22, training perplexity 67.75\n",
      "epoch 57, 300/451 batches, training loss 4.13, training perplexity 62.36\n",
      "epoch 57, 400/451 batches, training loss 4.06, training perplexity 58.07\n",
      "\n",
      "epoch 57, validation loss 4.87, validation perplexity 130.11\n",
      "\n",
      "epoch 58, 100/451 batches, training loss 4.24, training perplexity 69.27\n",
      "epoch 58, 200/451 batches, training loss 4.22, training perplexity 67.76\n",
      "epoch 58, 300/451 batches, training loss 4.13, training perplexity 62.07\n",
      "epoch 58, 400/451 batches, training loss 4.06, training perplexity 58.04\n",
      "\n",
      "epoch 58, validation loss 4.87, validation perplexity 130.13\n",
      "\n",
      "epoch 59, 100/451 batches, training loss 4.24, training perplexity 69.44\n",
      "epoch 59, 200/451 batches, training loss 4.22, training perplexity 67.75\n",
      "epoch 59, 300/451 batches, training loss 4.13, training perplexity 62.26\n",
      "epoch 59, 400/451 batches, training loss 4.06, training perplexity 58.07\n",
      "\n",
      "epoch 59, validation loss 4.87, validation perplexity 130.13\n",
      "\n",
      "epoch 60, 100/451 batches, training loss 4.23, training perplexity 69.01\n",
      "epoch 60, 200/451 batches, training loss 4.21, training perplexity 67.54\n",
      "epoch 60, 300/451 batches, training loss 4.13, training perplexity 62.13\n",
      "epoch 60, 400/451 batches, training loss 4.06, training perplexity 57.85\n",
      "\n",
      "epoch 60, validation loss 4.87, validation perplexity 130.11\n",
      "\n",
      "epoch 61, 100/451 batches, training loss 4.24, training perplexity 69.12\n",
      "epoch 61, 200/451 batches, training loss 4.22, training perplexity 67.83\n",
      "epoch 61, 300/451 batches, training loss 4.13, training perplexity 62.16\n",
      "epoch 61, 400/451 batches, training loss 4.06, training perplexity 58.01\n",
      "\n",
      "epoch 61, validation loss 4.87, validation perplexity 130.10\n",
      "\n",
      "epoch 62, 100/451 batches, training loss 4.24, training perplexity 69.26\n",
      "epoch 62, 200/451 batches, training loss 4.22, training perplexity 67.72\n",
      "epoch 62, 300/451 batches, training loss 4.13, training perplexity 62.37\n",
      "epoch 62, 400/451 batches, training loss 4.06, training perplexity 58.00\n",
      "\n",
      "epoch 62, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 63, 100/451 batches, training loss 4.24, training perplexity 69.07\n",
      "epoch 63, 200/451 batches, training loss 4.22, training perplexity 67.78\n",
      "epoch 63, 300/451 batches, training loss 4.13, training perplexity 62.12\n",
      "epoch 63, 400/451 batches, training loss 4.06, training perplexity 58.00\n",
      "\n",
      "epoch 63, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 64, 100/451 batches, training loss 4.24, training perplexity 69.13\n",
      "epoch 64, 200/451 batches, training loss 4.22, training perplexity 67.88\n",
      "epoch 64, 300/451 batches, training loss 4.13, training perplexity 62.27\n",
      "epoch 64, 400/451 batches, training loss 4.06, training perplexity 57.78\n",
      "\n",
      "epoch 64, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 65, 100/451 batches, training loss 4.23, training perplexity 68.97\n",
      "epoch 65, 200/451 batches, training loss 4.21, training perplexity 67.61\n",
      "epoch 65, 300/451 batches, training loss 4.13, training perplexity 62.15\n",
      "epoch 65, 400/451 batches, training loss 4.06, training perplexity 57.93\n",
      "\n",
      "epoch 65, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 66, 100/451 batches, training loss 4.24, training perplexity 69.27\n",
      "epoch 66, 200/451 batches, training loss 4.21, training perplexity 67.50\n",
      "epoch 66, 300/451 batches, training loss 4.13, training perplexity 62.21\n",
      "epoch 66, 400/451 batches, training loss 4.06, training perplexity 58.07\n",
      "\n",
      "epoch 66, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 67, 100/451 batches, training loss 4.24, training perplexity 69.34\n",
      "epoch 67, 200/451 batches, training loss 4.22, training perplexity 67.80\n",
      "epoch 67, 300/451 batches, training loss 4.13, training perplexity 62.19\n",
      "epoch 67, 400/451 batches, training loss 4.06, training perplexity 58.17\n",
      "\n",
      "epoch 67, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 68, 100/451 batches, training loss 4.24, training perplexity 69.10\n",
      "epoch 68, 200/451 batches, training loss 4.21, training perplexity 67.62\n",
      "epoch 68, 300/451 batches, training loss 4.13, training perplexity 62.28\n",
      "epoch 68, 400/451 batches, training loss 4.06, training perplexity 57.87\n",
      "\n",
      "epoch 68, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 69, 100/451 batches, training loss 4.24, training perplexity 69.25\n",
      "epoch 69, 200/451 batches, training loss 4.21, training perplexity 67.66\n",
      "epoch 69, 300/451 batches, training loss 4.13, training perplexity 62.06\n",
      "epoch 69, 400/451 batches, training loss 4.06, training perplexity 57.97\n",
      "\n",
      "epoch 69, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 70, 100/451 batches, training loss 4.24, training perplexity 69.23\n",
      "epoch 70, 200/451 batches, training loss 4.21, training perplexity 67.63\n",
      "epoch 70, 300/451 batches, training loss 4.13, training perplexity 62.27\n",
      "epoch 70, 400/451 batches, training loss 4.06, training perplexity 58.06\n",
      "\n",
      "epoch 70, validation loss 4.87, validation perplexity 130.09\n",
      "\n",
      "epoch 71, 100/451 batches, training loss 4.24, training perplexity 69.32\n",
      "epoch 71, 200/451 batches, training loss 4.21, training perplexity 67.60\n",
      "epoch 71, 300/451 batches, training loss 4.13, training perplexity 62.02\n",
      "epoch 71, 400/451 batches, training loss 4.06, training perplexity 57.90\n",
      "\n",
      "epoch 71, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 72, 100/451 batches, training loss 4.24, training perplexity 69.13\n",
      "epoch 72, 200/451 batches, training loss 4.22, training perplexity 67.82\n",
      "epoch 72, 300/451 batches, training loss 4.13, training perplexity 62.18\n",
      "epoch 72, 400/451 batches, training loss 4.06, training perplexity 58.00\n",
      "\n",
      "epoch 72, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 73, 100/451 batches, training loss 4.23, training perplexity 68.95\n",
      "epoch 73, 200/451 batches, training loss 4.21, training perplexity 67.56\n",
      "epoch 73, 300/451 batches, training loss 4.13, training perplexity 62.13\n",
      "epoch 73, 400/451 batches, training loss 4.06, training perplexity 57.86\n",
      "\n",
      "epoch 73, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 74, 100/451 batches, training loss 4.23, training perplexity 69.06\n",
      "epoch 74, 200/451 batches, training loss 4.21, training perplexity 67.49\n",
      "epoch 74, 300/451 batches, training loss 4.13, training perplexity 62.05\n",
      "epoch 74, 400/451 batches, training loss 4.06, training perplexity 57.98\n",
      "\n",
      "epoch 74, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 75, 100/451 batches, training loss 4.24, training perplexity 69.09\n",
      "epoch 75, 200/451 batches, training loss 4.21, training perplexity 67.63\n",
      "epoch 75, 300/451 batches, training loss 4.13, training perplexity 61.93\n",
      "epoch 75, 400/451 batches, training loss 4.06, training perplexity 58.11\n",
      "\n",
      "epoch 75, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 76, 100/451 batches, training loss 4.24, training perplexity 69.23\n",
      "epoch 76, 200/451 batches, training loss 4.22, training perplexity 67.71\n",
      "epoch 76, 300/451 batches, training loss 4.13, training perplexity 62.14\n",
      "epoch 76, 400/451 batches, training loss 4.06, training perplexity 57.90\n",
      "\n",
      "epoch 76, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 77, 100/451 batches, training loss 4.24, training perplexity 69.18\n",
      "epoch 77, 200/451 batches, training loss 4.22, training perplexity 67.78\n",
      "epoch 77, 300/451 batches, training loss 4.13, training perplexity 62.08\n",
      "epoch 77, 400/451 batches, training loss 4.06, training perplexity 57.85\n",
      "\n",
      "epoch 77, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 78, 100/451 batches, training loss 4.24, training perplexity 69.19\n",
      "epoch 78, 200/451 batches, training loss 4.21, training perplexity 67.59\n",
      "epoch 78, 300/451 batches, training loss 4.13, training perplexity 62.02\n",
      "epoch 78, 400/451 batches, training loss 4.06, training perplexity 57.96\n",
      "\n",
      "epoch 78, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 79, 100/451 batches, training loss 4.24, training perplexity 69.35\n",
      "epoch 79, 200/451 batches, training loss 4.22, training perplexity 67.70\n",
      "epoch 79, 300/451 batches, training loss 4.13, training perplexity 62.20\n",
      "epoch 79, 400/451 batches, training loss 4.06, training perplexity 57.87\n",
      "\n",
      "epoch 79, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 80, 100/451 batches, training loss 4.24, training perplexity 69.46\n",
      "epoch 80, 200/451 batches, training loss 4.21, training perplexity 67.60\n",
      "epoch 80, 300/451 batches, training loss 4.13, training perplexity 62.10\n",
      "epoch 80, 400/451 batches, training loss 4.06, training perplexity 57.84\n",
      "\n",
      "epoch 80, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 81, 100/451 batches, training loss 4.24, training perplexity 69.27\n",
      "epoch 81, 200/451 batches, training loss 4.22, training perplexity 67.81\n",
      "epoch 81, 300/451 batches, training loss 4.13, training perplexity 62.21\n",
      "epoch 81, 400/451 batches, training loss 4.06, training perplexity 57.96\n",
      "\n",
      "epoch 81, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 82, 100/451 batches, training loss 4.24, training perplexity 69.12\n",
      "epoch 82, 200/451 batches, training loss 4.22, training perplexity 67.77\n",
      "epoch 82, 300/451 batches, training loss 4.13, training perplexity 62.29\n",
      "epoch 82, 400/451 batches, training loss 4.06, training perplexity 58.18\n",
      "\n",
      "epoch 82, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 83, 100/451 batches, training loss 4.23, training perplexity 68.99\n",
      "epoch 83, 200/451 batches, training loss 4.21, training perplexity 67.48\n",
      "epoch 83, 300/451 batches, training loss 4.13, training perplexity 62.23\n",
      "epoch 83, 400/451 batches, training loss 4.06, training perplexity 58.03\n",
      "\n",
      "epoch 83, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 84, 100/451 batches, training loss 4.24, training perplexity 69.16\n",
      "epoch 84, 200/451 batches, training loss 4.21, training perplexity 67.39\n",
      "epoch 84, 300/451 batches, training loss 4.13, training perplexity 62.08\n",
      "epoch 84, 400/451 batches, training loss 4.06, training perplexity 58.13\n",
      "\n",
      "epoch 84, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 85, 100/451 batches, training loss 4.23, training perplexity 69.01\n",
      "epoch 85, 200/451 batches, training loss 4.22, training perplexity 67.74\n",
      "epoch 85, 300/451 batches, training loss 4.13, training perplexity 62.17\n",
      "epoch 85, 400/451 batches, training loss 4.06, training perplexity 57.75\n",
      "\n",
      "epoch 85, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 86, 100/451 batches, training loss 4.23, training perplexity 69.03\n",
      "epoch 86, 200/451 batches, training loss 4.21, training perplexity 67.51\n",
      "epoch 86, 300/451 batches, training loss 4.12, training perplexity 61.76\n",
      "epoch 86, 400/451 batches, training loss 4.06, training perplexity 57.81\n",
      "\n",
      "epoch 86, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 87, 100/451 batches, training loss 4.24, training perplexity 69.21\n",
      "epoch 87, 200/451 batches, training loss 4.21, training perplexity 67.68\n",
      "epoch 87, 300/451 batches, training loss 4.13, training perplexity 62.23\n",
      "epoch 87, 400/451 batches, training loss 4.06, training perplexity 58.04\n",
      "\n",
      "epoch 87, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 88, 100/451 batches, training loss 4.24, training perplexity 69.16\n",
      "epoch 88, 200/451 batches, training loss 4.21, training perplexity 67.66\n",
      "epoch 88, 300/451 batches, training loss 4.13, training perplexity 62.13\n",
      "epoch 88, 400/451 batches, training loss 4.06, training perplexity 57.72\n",
      "\n",
      "epoch 88, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 89, 100/451 batches, training loss 4.24, training perplexity 69.13\n",
      "epoch 89, 200/451 batches, training loss 4.21, training perplexity 67.68\n",
      "epoch 89, 300/451 batches, training loss 4.13, training perplexity 62.19\n",
      "epoch 89, 400/451 batches, training loss 4.06, training perplexity 57.86\n",
      "\n",
      "epoch 89, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 90, 100/451 batches, training loss 4.24, training perplexity 69.10\n",
      "epoch 90, 200/451 batches, training loss 4.21, training perplexity 67.52\n",
      "epoch 90, 300/451 batches, training loss 4.13, training perplexity 62.32\n",
      "epoch 90, 400/451 batches, training loss 4.06, training perplexity 57.89\n",
      "\n",
      "epoch 90, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 91, 100/451 batches, training loss 4.24, training perplexity 69.25\n",
      "epoch 91, 200/451 batches, training loss 4.21, training perplexity 67.69\n",
      "epoch 91, 300/451 batches, training loss 4.13, training perplexity 62.23\n",
      "epoch 91, 400/451 batches, training loss 4.06, training perplexity 57.95\n",
      "\n",
      "epoch 91, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 92, 100/451 batches, training loss 4.24, training perplexity 69.36\n",
      "epoch 92, 200/451 batches, training loss 4.22, training perplexity 67.71\n",
      "epoch 92, 300/451 batches, training loss 4.13, training perplexity 62.31\n",
      "epoch 92, 400/451 batches, training loss 4.06, training perplexity 57.82\n",
      "\n",
      "epoch 92, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 93, 100/451 batches, training loss 4.24, training perplexity 69.19\n",
      "epoch 93, 200/451 batches, training loss 4.21, training perplexity 67.54\n",
      "epoch 93, 300/451 batches, training loss 4.13, training perplexity 61.94\n",
      "epoch 93, 400/451 batches, training loss 4.06, training perplexity 57.97\n",
      "\n",
      "epoch 93, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 94, 100/451 batches, training loss 4.24, training perplexity 69.41\n",
      "epoch 94, 200/451 batches, training loss 4.21, training perplexity 67.57\n",
      "epoch 94, 300/451 batches, training loss 4.13, training perplexity 62.17\n",
      "epoch 94, 400/451 batches, training loss 4.06, training perplexity 58.06\n",
      "\n",
      "epoch 94, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 95, 100/451 batches, training loss 4.23, training perplexity 69.00\n",
      "epoch 95, 200/451 batches, training loss 4.21, training perplexity 67.54\n",
      "epoch 95, 300/451 batches, training loss 4.13, training perplexity 62.20\n",
      "epoch 95, 400/451 batches, training loss 4.06, training perplexity 57.94\n",
      "\n",
      "epoch 95, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 96, 100/451 batches, training loss 4.23, training perplexity 68.95\n",
      "epoch 96, 200/451 batches, training loss 4.21, training perplexity 67.56\n",
      "epoch 96, 300/451 batches, training loss 4.13, training perplexity 62.13\n",
      "epoch 96, 400/451 batches, training loss 4.06, training perplexity 58.05\n",
      "\n",
      "epoch 96, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 97, 100/451 batches, training loss 4.24, training perplexity 69.22\n",
      "epoch 97, 200/451 batches, training loss 4.21, training perplexity 67.64\n",
      "epoch 97, 300/451 batches, training loss 4.13, training perplexity 62.20\n",
      "epoch 97, 400/451 batches, training loss 4.06, training perplexity 58.13\n",
      "\n",
      "epoch 97, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 98, 100/451 batches, training loss 4.23, training perplexity 68.97\n",
      "epoch 98, 200/451 batches, training loss 4.21, training perplexity 67.64\n",
      "epoch 98, 300/451 batches, training loss 4.13, training perplexity 62.11\n",
      "epoch 98, 400/451 batches, training loss 4.06, training perplexity 58.08\n",
      "\n",
      "epoch 98, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 99, 100/451 batches, training loss 4.24, training perplexity 69.12\n",
      "epoch 99, 200/451 batches, training loss 4.21, training perplexity 67.56\n",
      "epoch 99, 300/451 batches, training loss 4.13, training perplexity 62.36\n",
      "epoch 99, 400/451 batches, training loss 4.06, training perplexity 57.78\n",
      "\n",
      "epoch 99, validation loss 4.87, validation perplexity 130.08\n",
      "\n",
      "epoch 100, 100/451 batches, training loss 4.24, training perplexity 69.08\n",
      "epoch 100, 200/451 batches, training loss 4.21, training perplexity 67.64\n",
      "epoch 100, 300/451 batches, training loss 4.13, training perplexity 61.89\n",
      "epoch 100, 400/451 batches, training loss 4.06, training perplexity 57.81\n",
      "\n",
      "epoch 100, validation loss 4.87, validation perplexity 130.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 100\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 4.78, testing perplexity 119.62\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
